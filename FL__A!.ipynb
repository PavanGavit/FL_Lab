{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWQfNfiZwgZG",
        "outputId": "8d46603d-d76c-40b6-9af9-c702b64b8e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/727.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m256.0/727.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m727.1/727.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 44.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q flwr tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile server.py\n",
        "import flwr as fl\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the strategy: we want to wait for at least 2 clients\n",
        "    strategy = fl.server.strategy.FedAvg(\n",
        "        min_fit_clients=2,        # Minimum number of clients to be sampled for the next round\n",
        "        min_available_clients=2,  # Minimum number of clients that need to be connected to start\n",
        "    )\n",
        "\n",
        "    # Start the server\n",
        "    print(\"Server starting... Waiting for 2 clients to connect.\")\n",
        "    fl.server.start_server(\n",
        "        server_address=\"0.0.0.0:8080\",\n",
        "        config=fl.server.ServerConfig(num_rounds=3),\n",
        "        strategy=strategy\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ho8pCdKUwlz6",
        "outputId": "a61b7852-ec5b-4b15-a367-256815dd2287"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile client.py\n",
        "import flwr as fl\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# 1. Parse Client ID from command line (to simulate different data partitions)\n",
        "# Example usage: python client.py 0  OR  python client.py 1\n",
        "if len(sys.argv) > 1:\n",
        "    client_id = int(sys.argv[1])\n",
        "else:\n",
        "    client_id = 0\n",
        "\n",
        "# 2. Data Pre-processing and Partitioning\n",
        "# We load the full dataset, but we slice it based on client_id\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Simple logic to split data into 2 partitions\n",
        "split_idx = len(x_train) // 2\n",
        "if client_id == 0:\n",
        "    # Client 0 gets the first half\n",
        "    x_train_local = x_train[:split_idx]\n",
        "    y_train_local = y_train[:split_idx]\n",
        "    print(f\"Client {client_id}: Using data indices 0 to {split_idx}\")\n",
        "else:\n",
        "    # Client 1 gets the second half\n",
        "    x_train_local = x_train[split_idx:]\n",
        "    y_train_local = y_train[split_idx:]\n",
        "    print(f\"Client {client_id}: Using data indices {split_idx} to {len(x_train)}\")\n",
        "\n",
        "# 3. Define the Model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 4. Define Flower Client\n",
        "class MyClient(fl.client.NumPyClient):\n",
        "    def get_parameters(self, config):\n",
        "        return model.get_weights()\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        model.set_weights(parameters)\n",
        "        # Train on the LOCAL partition\n",
        "        model.fit(x_train_local, y_train_local, epochs=1, batch_size=32, verbose=0)\n",
        "        return model.get_weights(), len(x_train_local), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        model.set_weights(parameters)\n",
        "        loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "        return loss, len(x_test), {\"accuracy\": accuracy}\n",
        "\n",
        "# 5. Connect to Server\n",
        "print(f\"Client {client_id} connecting to server...\")\n",
        "fl.client.start_numpy_client(server_address=\"127.0.0.1:8080\", client=MyClient())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dbj9Bp3woL_",
        "outputId": "5020c74f-3de7-4c7f-f030-e58d1ed4346f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing client.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "# 1. Start Server in the background\n",
        "print(\"ğŸš€ Starting Server in background...\")\n",
        "server_process = subprocess.Popen([\"python\", \"server.py\"])\n",
        "time.sleep(3) # Wait for server to initialize\n",
        "\n",
        "# 2. Start Client 0 in the background (Uses first half of data)\n",
        "print(\"ğŸ‘¤ Starting Client 0 in background...\")\n",
        "client0_process = subprocess.Popen([\"python\", \"client.py\", \"0\"])\n",
        "\n",
        "# 3. Start Client 1 in the foreground (Uses second half of data)\n",
        "# We run this with ! so we can see the output logs in real-time\n",
        "print(\"ğŸ‘¤ Starting Client 1 in foreground...\")\n",
        "!python client.py 1\n",
        "\n",
        "# 4. Cleanup: Kill processes after training is done\n",
        "print(\"\\nâœ… Training Complete. Shutting down background processes.\")\n",
        "server_process.kill()\n",
        "client0_process.kill()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG4lUhhLwqSX",
        "outputId": "2160c0a9-5252-4d72-83d5-c4d7f357d05c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting Server in background...\n",
            "ğŸ‘¤ Starting Client 0 in background...\n",
            "ğŸ‘¤ Starting Client 1 in foreground...\n",
            "2026-01-21 08:56:23.744014: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-01-21 08:56:23.750218: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-01-21 08:56:23.774158: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768985783.815358     935 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768985783.827191     935 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768985783.855707     935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768985783.855757     935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768985783.855767     935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768985783.855775     935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 08:56:23.863936: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Client 1: Using data indices 30000 to 60000\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "2026-01-21 08:56:35.696067: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Client 1 connecting to server...\n",
            "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. \n",
            "\tInstead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: \n",
            "\tflwr.client.start_client(\n",
            "\t\tserver_address='<IP>:<PORT>',\n",
            "\t\tclient=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object\n",
            "\t)\n",
            "\tUsing `start_numpy_client()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
            "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
            "\n",
            "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
            "\n",
            "\tTo view all available options, run:\n",
            "\n",
            "\t\t$ flower-supernode --help\n",
            "\n",
            "\tUsing `start_client()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/client.py\", line 57, in <module>\n",
            "    fl.client.start_numpy_client(server_address=\"127.0.0.1:8080\", client=MyClient())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/compat/client/app.py\", line 625, in start_numpy_client\n",
            "    start_client(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/compat/client/app.py\", line 184, in start_client\n",
            "    start_client_internal(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/compat/client/app.py\", line 395, in start_client_internal\n",
            "    message = receive()\n",
            "              ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/compat/client/grpc_client/connection.py\", line 142, in receive\n",
            "    proto = next(server_message_iterator)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/grpc/_channel.py\", line 538, in __next__\n",
            "    return self._next()\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/grpc/_channel.py\", line 962, in _next\n",
            "    raise self\n",
            "grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n",
            "\tstatus = StatusCode.UNAVAILABLE\n",
            "\tdetails = \"failed to connect to all addresses; last error: INTERNAL: ipv4:127.0.0.1:8080: Trying to connect an http1.x server (HTTP status 400)\"\n",
            "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: INTERNAL: ipv4:127.0.0.1:8080: Trying to connect an http1.x server (HTTP status 400)\", grpc_status:14}\"\n",
            ">\n",
            "\n",
            "âœ… Training Complete. Shutting down background processes.\n"
          ]
        }
      ]
    }
  ]
}